#!/usr/bin/env python3
"""
memory-compounding.py â€” Memory reflection and insight extraction

åŸºäºæ–¯å¦ç¦ Generative Agents çš„åæ€æœºåˆ¶ï¼š
- çç¢è®°å¿† â†’ LLM æç‚¼æ´å¯Ÿ â†’ å­˜å…¥ insights/

ç”¨æ³•ï¼š
    python3 memory-compounding.py                     # æ ‡è®°æ˜¨å¤©
    python3 memory-compounding.py --date 2026-02-05   # æ ‡è®°æŒ‡å®šæ—¥æœŸ
    python3 memory-compounding.py --list              # åˆ—å‡ºå¾…å¤„ç†
    python3 memory-compounding.py --process           # å¤„ç†æ‰€æœ‰ pending (è¾“å‡ºæŒ‡ä»¤)
    python3 memory-compounding.py --extract DATE      # æå–æŒ‡å®šæ—¥æœŸçš„æ´å¯Ÿ

Cron Integration:
    3 AM: memory-compounding.py --mark-pending        # æ ‡è®°
    4 AM: cron dispatcher triggers sub-agent          # å¤„ç†
"""

import argparse
import json
import os
from datetime import datetime, timedelta
from pathlib import Path
import sys
import re

# Configure this path for your setup
MEMORY_DIR = Path(os.environ.get("MEMORY_DIR", os.path.expanduser("~/memory")))
PENDING_DIR = MEMORY_DIR / ".pending"
INSIGHTS_DIR = MEMORY_DIR / "insights"

# æ´å¯Ÿæå– prompt (åŸºäº Stanford Generative Agents)
REFLECTION_PROMPT = """You are extracting insights from a daily memory log.

## Task
Read the log and extract a STRUCTURED summary with these MANDATORY sections.
Each section MUST be present even if the content is minimal.

## Output Format (ALL sections required)
```markdown
## {DATE} Insights

### Session Intent
[What was the main focus today? 1-2 sentences max]

### Files Modified
[List ALL files that were created/edited/deleted. If none mentioned, write "None recorded"]
- path/to/file: what changed

### Decisions Made
[Important choices with rationale]
- Decision: [what] â€” Reason: [why]

### Lessons Learned
[Mistakes â†’ fixes, what went wrong and how to avoid]
- **é—®é¢˜**: [what went wrong]
- **åŸå› **: [root cause]
- **ä¿®å¤**: [how to prevent]

### Patterns
[Recurring solutions or workflows that worked]
- **[pattern name]**: [description with concrete example]

### Open Items
[Unfinished tasks, things to follow up]
- [ ] item

### Statistics
- Log length: {chars} chars
- Decisions: N
- Lessons: N
- Files modified: N
```

## Extraction Rules
- Compress 10:1 (1000 chars log â†’ ~100 chars insight)
- Keep [P0] markers for permanent rules
- Include concrete commands/paths/code when mentioned
- Skip trivial chatter, focus on learnings
- Output in the language of the log content
- **Files Modified is CRITICAL** â€” scan for any path like ~/*, /Users/*, *.py, *.sh, *.md

## Daily Log to Process
{log_content}
"""

def get_yesterday():
    return (datetime.now() - timedelta(days=1)).strftime("%Y-%m-%d")

def is_compounded(filepath):
    """Check if file has compounded marker."""
    try:
        content = filepath.read_text()
        return "<!-- compounded" in content
    except:
        return False

def mark_pending(date_str=None):
    """Mark a daily log as pending for LLM processing."""
    date_str = date_str or get_yesterday()
    log_file = MEMORY_DIR / f"{date_str}.md"
    
    print(f"ğŸ“… Checking {date_str}...")
    
    if not log_file.exists():
        print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {log_file}")
        return False
    
    if is_compounded(log_file):
        print(f"â­ï¸ å·²å¤„ç†è¿‡")
        return False
    
    content = log_file.read_text()
    if len(content.strip()) < 100:
        print(f"â­ï¸ å†…å®¹å¤ªå°‘ ({len(content)} chars)")
        return False
    
    # Check if already pending
    PENDING_DIR.mkdir(exist_ok=True)
    pending_file = PENDING_DIR / f"{date_str}.pending"
    
    if pending_file.exists():
        print(f"â­ï¸ å·²åœ¨å¾…å¤„ç†é˜Ÿåˆ—")
        return False
    
    # Create pending marker
    pending_file.write_text(json.dumps({
        "date": date_str,
        "file": str(log_file),
        "chars": len(content),
        "created": datetime.now().isoformat()
    }))
    
    print(f"âœ… æ ‡è®°å¾…å¤„ç†: {pending_file.name}")
    return True

def list_pending():
    """List all pending files."""
    if not PENDING_DIR.exists():
        return []
    return sorted(PENDING_DIR.glob("*.pending"))

def get_pending_info():
    """Get info about pending files."""
    pending = list_pending()
    result = []
    for p in pending:
        try:
            data = json.loads(p.read_text())
            result.append(data)
        except:
            pass
    return result

def prepare_extraction(date_str):
    """Prepare extraction prompt for a specific date."""
    log_file = MEMORY_DIR / f"{date_str}.md"
    
    if not log_file.exists():
        print(f"âŒ æ—¥å¿—æ–‡ä»¶ä¸å­˜åœ¨: {log_file}")
        return None
    
    content = log_file.read_text()
    
    # Truncate if too long (keep most recent)
    max_chars = 15000
    if len(content) > max_chars:
        content = f"[...truncated...]\n\n{content[-max_chars:]}"
    
    prompt = REFLECTION_PROMPT.replace("{DATE}", date_str).replace("{log_content}", content)
    return prompt

def save_insights(date_str, insights_text):
    """Save extracted insights to the monthly insights file."""
    INSIGHTS_DIR.mkdir(exist_ok=True)
    
    # Determine monthly file
    month = date_str[:7]  # 2026-02
    insights_file = INSIGHTS_DIR / f"{month}.md"
    
    # Read existing or create header
    if insights_file.exists():
        existing = insights_file.read_text()
    else:
        existing = f"# Insights - {datetime.strptime(month, '%Y-%m').strftime('%B %Y')}\n\n"
    
    # Append new insights
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M")
    new_content = existing + f"\n{insights_text}\n\n*LLM æå–äº {timestamp}*\n\n---\n"
    
    insights_file.write_text(new_content)
    print(f"âœ… æ´å¯Ÿå·²ä¿å­˜åˆ° {insights_file}")
    return True

def mark_compounded(date_str):
    """Mark a log file as compounded."""
    log_file = MEMORY_DIR / f"{date_str}.md"
    
    if not log_file.exists():
        return False
    
    content = log_file.read_text()
    
    # Add compounded marker at the top
    marker = f"<!-- compounded: {datetime.now().strftime('%Y-%m-%d')} -->\n"
    if not content.startswith("<!--"):
        content = marker + content
    else:
        # Insert after existing front matter
        content = marker + content
    
    log_file.write_text(content)
    
    # Remove pending file
    pending_file = PENDING_DIR / f"{date_str}.pending"
    if pending_file.exists():
        pending_file.unlink()
    
    print(f"âœ… å·²æ ‡è®° {date_str} ä¸ºå·²å¤„ç†")
    return True

def generate_process_instructions():
    """Generate instructions for sub-agent to process pending files."""
    pending = get_pending_info()
    
    if not pending:
        print("ğŸ“­ æ— å¾…å¤„ç†æ–‡ä»¶")
        return None
    
    print(f"ğŸ“‹ æ‰¾åˆ° {len(pending)} ä¸ªå¾…å¤„ç†æ–‡ä»¶:")
    for p in pending:
        print(f"  - {p['date']} ({p['chars']} chars)")
    
    # Generate sub-agent task
    dates = [p['date'] for p in pending]
    task = f"""Memory Reflection Task:

å¤„ç†ä»¥ä¸‹æ—¥æœŸçš„æ—¥å¿—:
{chr(10).join(f'- {d}' for d in dates)}

å¯¹æ¯ä¸ªæ—¥æœŸæ‰§è¡Œ:
1. è¿è¡Œ: python3 memory-compounding.py --extract {dates[0]}
2. æ ¹æ®è¾“å‡ºçš„ prompt æå–æ´å¯Ÿ (3-7 æ¡)
3. å°†æ´å¯Ÿå†™å…¥ memory/insights/YYYY-MM.md
4. è¿è¡Œ: python3 memory-compounding.py --done {dates[0]}

è§„åˆ™:
- 10:1 å‹ç¼©æ¯”
- ä¿ç•™ [P0] æ ‡è®°
- åŒ…å«å…·ä½“å‘½ä»¤/ä»£ç 
- ä½¿ç”¨æ—¥å¿—çš„è¯­è¨€ (ä¸­/è‹±)
"""
    print("\n" + "="*50)
    print("ğŸ“¤ Sub-agent Task:")
    print("="*50)
    print(task)
    return task

def main():
    parser = argparse.ArgumentParser(description="Memory reflection and insight extraction")
    parser.add_argument("--date", help="æŒ‡å®šæ—¥æœŸ YYYY-MM-DD (é»˜è®¤æ˜¨å¤©)")
    parser.add_argument("--list", action="store_true", help="åˆ—å‡ºå¾…å¤„ç†æ–‡ä»¶")
    parser.add_argument("--mark-pending", action="store_true", help="æ ‡è®°å¾…å¤„ç† (é»˜è®¤è¡Œä¸º)")
    parser.add_argument("--process", action="store_true", help="ç”Ÿæˆå¤„ç†æŒ‡ä»¤")
    parser.add_argument("--extract", metavar="DATE", help="è¾“å‡ºæŒ‡å®šæ—¥æœŸçš„æå– prompt")
    parser.add_argument("--done", metavar="DATE", help="æ ‡è®°æŒ‡å®šæ—¥æœŸä¸ºå·²å¤„ç†")
    parser.add_argument("--batch-mark", type=int, metavar="DAYS", help="æ‰¹é‡æ ‡è®°è¿‡å» N å¤©")
    args = parser.parse_args()
    
    if args.list:
        pending = list_pending()
        if pending:
            print("ğŸ“‹ å¾…å¤„ç†æ–‡ä»¶:")
            for p in pending:
                data = json.loads(p.read_text())
                print(f"  - {data['date']} ({data['chars']} chars)")
        else:
            print("ğŸ“­ æ— å¾…å¤„ç†")
        return
    
    if args.process:
        generate_process_instructions()
        return
    
    if args.extract:
        prompt = prepare_extraction(args.extract)
        if prompt:
            print(prompt)
        return
    
    if args.done:
        mark_compounded(args.done)
        return
    
    if args.batch_mark:
        print(f"ğŸ“… æ‰¹é‡æ ‡è®°è¿‡å» {args.batch_mark} å¤©...")
        marked = 0
        for i in range(1, args.batch_mark + 1):
            date_str = (datetime.now() - timedelta(days=i)).strftime("%Y-%m-%d")
            if mark_pending(date_str):
                marked += 1
        print(f"\nâœ… æ ‡è®°äº† {marked} ä¸ªæ–‡ä»¶")
        return
    
    # Default: mark yesterday (or specified date)
    mark_pending(args.date)

if __name__ == "__main__":
    main()
